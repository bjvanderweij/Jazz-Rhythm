\chapter{Method}
\label{sec:method}

In this chapter, the complete framework will be described. Section \ref{sec:parser} will introduce a parser that constructs valid rhythmic structures. Section \ref{sec:rejection} will describe the probabilistic elements of the parser. These are elaborated in section \ref{sec:prior} and \ref{sec:likelihood} where respectively the rhythm and expression model are discussed. Section \ref{sec:corpus} will introduce the jazz corpus. Section \ref{sec:training} will describe how the rhythm and expression model are trained on the corpus. Finally, section \ref{sec:implementation} will describe a few implementation details of the parser.

\section{Parsing Rhythms}
\label{sec:parser}

We represent the performance $P$ of a rhythm as series of note onset times. 
\begin{equation}
\label{eq:performance}
P = [\mathrm{On}_0, \mathrm{On}_1, \cdots, \mathrm{On}_n]
\end{equation}
A subdivision tree $R$ is a hierarchical representation of rhythmic structure.

The approach presented here generates the most likely rhythmic structure $R$ underlying a performance $P$. During this process the parser considers all possible hypotheses of sub spans of $P$ and retains the most likely hypotheses, while rejecting the unlikely ones. We think this is computationally feasible because we assume that over a few notes, only a small number of hypotheses are worth considering. In this section we will first describe how structurally sound rhythmic analyses are generated. After that, we will outline a Bayesian model that defines how we determine whether a hypothesis is likely.

The parser we use is a slightly modified stochastic CKY chart parser \citep{Younger1967recognition}. The full algorithm and modifications are given in appendix \ref{app:parser}. A small context-free grammar augmented with some constraints will be used to generate subdivision trees. The grammar below constructs subdivision trees from onsets ($\bullet$) and ties ($*$).
\begin{align}
R &\rightarrow R\: R\\ \notag
R &\rightarrow R\: R\: R\\ \notag
R &\rightarrow \bullet\\ \notag
R &\rightarrow * \notag
\end{align}
Every rhythmic structure, denoted by the $R$ symbol corresponds to some metrical duration. A rule expansion for some $R$ in the grammar above corresponds to subdividing the metrical duration of $R$ into the number of symbols the rule expands into. For this study, we will restrict ourselves to duple and triple subdivisions.

The CKY parser only accepts grammars that are given in the so-called Chomsky normal form (CNF). That is, all rules should be of the form $A \rightarrow B, C$ or $A \rightarrow \alpha$, where $A$, $B$ and $C$ are non-terminal symbols and $\alpha$ is a terminal symbol. Converting the grammar above to CNF results in the following grammar:
\begin{align}
\label{eq:grammar_cnf}
R &\rightarrow R\: R\\ \notag
R &\rightarrow R\: R'\\ \notag
R' &\rightarrow R\: R\\ \notag
R &\rightarrow \bullet\\ \notag
R &\rightarrow * \notag
\end{align}

Two constraints are necessary to prevent the parser from generating invalid rhythmical structures. These constraints are: (1) Any set of two or three metrical durations are not allowed to combine if the first one expands directly to an onset and the others do not recursively expand to an onset; (2) Metrical durations are not allowed to combine if none of them recursively contains an onset.

\begin{figure}
\centering
\subfloat[]{
\label{fig:constraints:a}
\parbox{0.2\textwidth}{
\Tree
[ .{$\frac{1}{1}$} [ .$\bullet$ ] [ .{$\frac{1}{2}$} [ .$\bullet$ ] [ .$*$ ] ] ]
}
$\mathlarger{\mathlarger{\mathlarger{\equiv}}}$
\parbox{0.2\textwidth}{
\Tree
[ .{$\frac{1}{1}$} [ .$\bullet$ ] [ .$\bullet$ ] ]
}
}
\qquad
\subfloat[]{
\label{fig:constraints:b}
\parbox{0.2\textwidth}{
\Tree
[ .{$\frac{1}{1}$} [ .{$\frac{1}{2}$} [ .$*$ ] [ .$*$ ] ] [ .$\bullet$ ] ] 
}
$\mathlarger{\mathlarger{\mathlarger{\equiv}}}$
\parbox{0.2\textwidth}{
\Tree
[ .{$\frac{1}{1}$} [ .$*$ ] [ .$\bullet$ ] ] 
}
}
\caption{Redundant tree structures.}
\label{fig:constraints}
\end{figure}

The first constraint prevents the parser from generating structures with an onset on the downbeat and a tie on the upbeat. An upbeat tied to a downbeat is redundant: If we tie an upbeat quarter note to a downbeat quarter note we get a half note and not two tied quarter notes. The second constraint prevents the parser from combining subdivision trees that do not contain onsets. Figure \ref{fig:constraints:a} illustrates the first constraint and figure \ref{fig:constraints:b} illustrates the second.

Because we do not know how many $*$ symbols are present in the input, we had to modify the parser to also consider inputs with $*$ symbols added in places where it would satisfy the two constraints above. See appendix \ref{app:parser} for details.
% Explanation of the chart parser?

At this point, the parser would generates all possible interpretations of an onset list of a certain length, which is an infinite amount. The next sections will describe how a probabilistic model is used to reject unlikely interpretations and retain the likely ones.

\section{Hypothesis Generation and Rejection}
\label{sec:rejection}

Subdivision trees generated by the parser can be seen as hypotheses about the rhythmic structure of (some sub-span of) the performance. In our Bayesian model, the likelihood of a hypotheses given a set of performed onsets is determined by two factors: how likely is it that the rhythmic structure generated the observed performance and how likely is the rhythmic structure itself. In other words, we want to find the \textit{posterior} probability $P(R|P)$, where $P$ is a performance, and $R$ is the rhythmic structure. We can formulate this as a generative model where
\begin{equation}
\label{eq:model}
P(R|P) \propto P(P|R)P(R).
\end{equation}
The posterior probability $P(R|P)$ of a rhythm $R$ given a performance $P$ is proportional to $P(P|R)$, the probability that $R$ generated performance $P$ times $P(R)$, the probability of $R$ itself. $P(P|R)$ is called the \textit{likelihood} of $R$ given $P$ and $P(R)$ is called the \textit{prior} probability of $R$.

Another way to refer to the prior and the likelihood is respectively as a rhythm model and an expression model. The rhythm model should reflect intuitions about rhythms, for example that long notes tend to fall on downbeats, that duple divisions are more likely than triple divisions, etcetera. The prior used in this study will be described in section \ref{sec:prior}

The expression model defines how and to what extent we expect onsets to deviate from their expected onsets. In our system, the expression model will be based on one observation, which we shall call the \textit{expression ratio}. The expression ratio is defined as the logarithmic ratio of downbeat length and upbeat length. In metronomic performances, we expect downbeats and upbeats to be of equal length and their ratio to one. In human performances however, the expression ratio will be a measure of expressive timing. At low levels, the expression ratio reflects local expressive timing. A slightly stretched downbeat at the quarter note level for example, will produce an expression ratio slightly above zero. At higher levels the expression ratio reflects global changes in tempo. For example, slowing down gradually will result in expression ratios slightly below zero on higher levels.

% So no tempo curves are needed

Finally, the chart parser should only keep track of a limited number of sensible hypotheses. We will restrict hypothesis maintained by the parser in two ways: First, the per-item likelihood of the hypothesis (see section \ref{sec:training}) should be higher than a certain threshold parameter. Second, after a cell has been filled with hypotheses by the parser (see appendix \ref{app:parser} for more details), hypotheses are ranked by their posterior probability and only the top-$n$ hypotheses are kept. This both of these techniques implement a technique called `beam search'. 

Both the rhythm and the expression model will be trained on the corpus of annotated jazz performances that was constructed for this study and which will be described in detail in section \ref{sec:corpus}.

\section{The Rhythm Model}
\label{sec:prior}

Our rhythm model will be a probabilistic context-free grammar (PCFG). A PCFG is a context-free grammar extended with probabilities for every rewrite rule. The probability of a syntax tree, produced by a PCFG can be derived by taking the product of every rule that was applied to construct the tree. In linguistics, PCFGs do not always assign probabilities to rules expanding to terminal symbols (words) since there are too many of them. In our case however, there are only two terminal symbols so we can assign probabilities to rules expanding to onsets or ties as well.

Note that there is only one non-terminal symbol in our grammar, namely $R$, so the probability of a rule expansion is given by:
\begin{equation}
P(R \rightarrow S) = P(S) = \frac{\mathrm{count}(S)}{N},
\end{equation}
where S is a string of symbols and N is the total number of $R$ symbols in the training set. 

The probability of a subdivision tree $R$ can be described as 
\begin{equation}
P(R) = \prod_{R' \rightarrow S \in R} P(R' \rightarrow S).
\end{equation}
where $R' \rightarrow S \in R$ refers to every rule expansion of some symbol $R'$ to a string of symbols $S$ that is recursively contained in $R$.


\section{The Expression Model}
\label{sec:likelihood}

This section will describe an expression model that represents expressive deviation as logarithmic ratios of downbeat and upbeat length. Section \ref{sec:training} will describe how this model is trained. Using different training methods, one model will be made `expression-aware' and the other will treat expression as noise.


\begin{figure}
\Tree
[ .{$\frac{1}{1}$} [ .{$\frac{1}{2}$} [ .$\bullet$ ] [ .$\bullet$ ] ] [ .$*$ ] ]
\caption{A simple subdivision tree.}
\label{fig:smalltree}
\end{figure}

Before turning to how we observe expression ratios, we will describe how hypotheses are represented. Subsequently, we will introduce the features used to predict expression ratios and finally we will introduce the \textsc{observations} function, which returns a set of features and expression ratios for any hypothesis.

In order to be able to say anything about the likelihood of an analysis, we need to have information about the onsets it contains. A subdivision tree represents rhythmic structure, but it does not keep track of the onsets associated with the structure. Therefore we will introduce a distinction between a subdivision tree or, rhythmic structure $R$, produced by the grammar in (\ref{eq:grammar_cnf}) and a \textit{hypothesis} $h$.

A subdivision tree is a hierarchical structure that can be represented as a nested list. The tree in figure \ref{fig:smalltree} for example, can also be written as $((\bullet, \bullet), *)$. Hypotheses have a similar structure but instead of onset symbols $\bullet$, contain actual onset times at the leaf nodes. We refer to the child nodes of an hypothesis as $h_i$ where $i$ is the beat position of the node so that $h_0$ is the downbeat and $h_i$, where $i>0$, are the upbeats. A hypothesis is said to \textit{govern} an onset if it recursively contains that onset. The number of child nodes of a hypothesis corresponds to its subdivision. We will call the child nodes of a hypothesis its beats. Since subdivision trees can combine measures and groups of measures into metrical units, a beat can govern multiple measures.

We will define three functions over hypotheses: \textsc{division}($h$), \textsc{onsets}($h$) and \textsc{beats}($h$). The \textsc{division}($h$) function counts the number of child nodes, or beats, of hypothesis $h$, which can be two or three in our implementation. The \textsc{onsets}($h$) function returns a list with length \textsc{division}($h$) which contains the onset time or a tie symbol for each beat in the hypothesis. The list of onsets is constructed by taking the downbeat of every child node. The downbeat of a hypothesis can be defined recursively:
\begin{equation}
\textsc{downbeat}(h) = 
\begin{cases}
   \textsc{downbeat}(h_0) & \mbox{if $h$ has child nodes}\\
   h & \mbox{otherwise}\\
  \end{cases}
\end{equation}
The \textsc{onsets} function may return lists containing $*$ symbols if some beats of a hypothesis contain ties. For the tree in figure \ref{fig:smalltree} for example, the \textsc{onsets} function returns the onset of the downbeat and a tie.

Finally, \textsc{beats}($h$) is a function that returns a list of expected onsets of every beat in $h$ for all hypotheses that govern more than one onset. If $h$ governs a single onset, the \textsc{beats} and \textsc{onsets} functions are equivalent. Predicting onset times is a bottom-up process which we will leave to appendix \ref{app:predicting} to explain.

If $h$ is an onset or tie, \textsc{division}($h$) will return zero and \textsc{beats}($h$) and \textsc{onsets}($h$)
will return $h$.

A top-down process will now determine the observed expression ratios and feature vectors that we mentioned earlier. This will be done by a function called \textsc{observations}, which, given some hypothesis, returns a vector of observations containing feature vector/expression ratio pairs. The likelihood of these observations given their feature vectors can be determined after we learn to map feature vectors to an expected expression ratio. This will be done by training our model on the jazz corpus as described in section \ref{sec:training}.

The feature vector will contain two features. We have already observed in section \ref{sec:rejection} that the expression ratio reflects different concepts as different levels, therefore one feature will be the \texttt{level} at which the expression ratio was observed. We will use another feature reflecting the \texttt{division} of the metrical unit in which the expression ratio was observed. The resulting feature vector is:
\begin{equation}
\label{eq:features}
\varphi = [\texttt{level}, \texttt{division}].
\end{equation}
\texttt{level} is defined bottom-up, so level one is the deepest level of the tree. Level is not to be confused with depth, the highest level of a tree equals the root node which has the shallowest depth.

For any hypothesis $h$, we can calculate the expression ratio given that we know the (estimated or actual) onset of the downbeat, $h_0$ and the (estimated or actual) next downbeat. Since a hypothesis can be divided into more than two units, there may be more than one expression ratio per hypothesis. We define the \textit{relative position} of a beat to be zero for the downbeat, one for the first upbeat and two for the second upbeat. The expression ratio for a hypothesis with division $d = \textsc{division}(h)$, onsets $O = \textsc{onsets}(h)$, predicted beats $B = \textsc{beats}(h)$, next downbeat $B_d$, beat onset $O_i$ is calculated in the following fashion:
\begin{align}
\label{eq:expression}
\mbox{expression ratio} &= \log\left(\frac{(O_i - B_0) / i}{(B_d - O_i) / (d - i)}\right) & \text{for every $i$ where $i > 0$ and $O_i \neq *$.}
\end{align}

The \textsc{observations} function is used to derive downbeat and next downbeat estimates for any hypothesis that contains more than one onset and all the hypothesis recursively contained by $h$. This process has been designed with one intuition in mind: that downbeat intervals provide the most reliable information about where onsets are to be expected. 

The \textsc{observations} function is a top-down recursive function. It is initialised with a hypothesis $h$, the downbeat of $h$ and the onset of next downbeat. Since we have not yet observed the next downbeat for the root-node hypothesis, we can only calculate the expected onset of the next downbeat. So for a hypothesis $h$, the algorithm is initialised as follows: 
\[\textsc{observations}(h, B_0, *, \textsc{division}(h) \times (B_1 - B_0)),\]
where $B = \textsc{beats}(h)$, the second argument $B_0$, is the downbeat, the third argument $*$, is onset of the next downbeat (which we do not know) and the last argument is the expected onset of the next downbeat.

The full \textsc{observations} function is given in algorithm \ref{alg:observations}. The first step of the algorithm is to initialise the subdivision of the hypothesis $d$, the list of beats $B$ and list of onsets $O$. The estimated duration $l$ of the hypothesis can now be calculated as the onset of the next downbeat minus the onset of the downbeat. Finally, the set of feature vector/expression ratio pairs $S$ is initialised as an empty set and the $*$ symbol is appended to the list of onsets.

Now the algorithm iterates through every beat in the hypothesis. For every beat position $i$ where $i > 0$ and $O_i \neq *$ the expression ratio is calculated as in equation \ref{eq:expression}. The requirements $i > 0$ and $O_i \neq *$ ensure that the expression ratio is only calculated for upbeats that contain actual onsets. 

For every beat position in $h$, the algorithm will calculate the downbeat and next downbeat for the nested hypothesis at that position, these values are stored in $b_{\mathrm{down}}$ and $b_{\mathrm{up}}$. 
For some beat position $i$, where $0 < i < d$, $b_{\mathrm{down}}$ is estimated by
\begin{equation}
\label{eq:beatonset}
b_{\mathrm{down}} = \mathrm{downbeat} + l * i/d.
\end{equation}
If an onset has been estimated by the combination process, which indicated by $B_i \neq *$, this onset is used instead. Since the combination process will always have estimated onsets for $h$s governing more than one onset, equation \ref{eq:beatonset} is only used when $h$ governs a single onset.

Given $b_{\mathrm{down}}$ the position of the next downbeat, $b_{\mathrm{up}}$ is estimated by:
\begin{equation}
b_{\mathrm{up}} = b_{\mathrm{down}} + l/d.
\end{equation}
This estimate should be equivalent to the onset at position $i+1$. If there is an onset at position $i+1$, this onset is preferred to the actual onset.

Finally, the recursive step of the algorithm calls \textsc{observations}($h_i, b_{\mathrm{down}}, b_{\mathrm{up}}$) for every nested hypothesis $h_i$ of $h$ where $0 \leq i < d$ and $\textsc{division}(h) > 0$.


\begin{algorithm}[h!]
\caption{Generate observations}
\label{alg:observations}
\begin{algorithmic}
\Function{observations}{$h$, downbeat, nextDownbeat, expected}
	\State $B \leftarrow$ \Call{beats}{$h$}
	\State $O \leftarrow$ \Call{onsets}{$h$}
	\State $d \leftarrow$ \Call{division}{$h$}
	\State $l \leftarrow$ nextDownbeat - downbeat
	\State $S \leftarrow \emptyset$
	\State \textbf{append} * \textbf{to} $O$
	\For {$i \leftarrow 0,d$}
		\If {$O_i \neq *$ \textbf{and} $i \neq 0$}
			\State $\varphi \leftarrow$ (\Call{depth}{$h$}, $d$)
			\State $r \leftarrow$ \Call{expression\textunderscore ratio}{downbeat, nextDownbeat, $B_i$, $i$, $d$}
			\State \textbf{append} ($\varphi$, r) \textbf{to} $S$
		\EndIf	
		\State $B' \leftarrow$ \Call{beats}{$h_i$}
		\State $O' \leftarrow$ \Call{onsets}{$h_i$}
		\If {\Call{division}{$h'$} $\neq 0$}
			\State $b_{\mathrm{down}} \leftarrow \mathrm{downbeat} + l * i/d$
			\If {$B_i \neq *$}
				\State $b_{\mathrm{down}} \leftarrow B_i$
			\EndIf
			\State $b_{\mathrm{up}} \leftarrow b_{\mathrm{down}} + l/d$
			\If{$O_{i+1} \neq *$}
				\State $b_{\mathrm{up}} \leftarrow O_{i+1}$
			\EndIf
			\State \textbf{append} \Call{observations}{$h_i, b_{\mathrm{down}}, b_{\mathrm{up}}$} \textbf{to} $S$
		\EndIf
	\EndFor
	\State \textbf{return} $S$
\EndFunction
\end{algorithmic}
\end{algorithm}


%Constraints of the corpusparser (a parser that only allows metronomic performances)

%Single note analysis constraints

%Constraint in adding ties


%Introducing triple divisions introduces ambiguities between triple divisions or more complicated constructions of duple divisions (that are quite unlikely).


\section{Data Preparation}
\label{sec:corpus}

To train the parser's rhythm model and expression model, a corpus of amateur jazz performances was prepared. The corpus contains jazz and latin standards that were scraped off the the web by \citet{Wild:10}. The performances are generally of good quality and are played in a relatively constant tempo. In its original form, the corpus was a set of multi-track MIDI files containing both tracks played by a human performer and tracks generated by a computer in metronomic time.

MIDI files represent music as a list of note-on and note-off events, corresponding to key presses and key releases. Every event has the parameters pitch, velocity and delta-time. Delta-time specifies the time between this events and the last one, pitch is the pitch of the key that was pressed, velocity is the velocity with which the key was pressed or released.

%Our parser will be trained and evaluated on monophonic jazz melodies, played by a human performer. Monophonic tracks that were likely to be played by humans were filtered out automatically. This was done by assuming that tracks played by humans contained a lot of variation in onset velocity and in inter-onset intervals due to imperfect timing. From this subset of performed tracks, tracks containing melodies where selected by hand.

After the filtering process, 20 tracks were left containing unique performances of 12 different melodies. These MIDI files where converted to note lists of the following format:
\begin{align*}
N &= [n_0, n_1, \cdots, n_N],\\
n_i &= (\mathrm{On}_i, \mathrm{Pitch}_i, \mathrm{Velocity}_i),
\end{align*}
where $N$ is a note list containing notes $n_0$ to $n_N$, $\mathrm{On}_i$, $\mathrm{Pitch}_i$ and $\mathrm{Velocity}_i$ is the onset time in micro seconds, pitch and velocity of the $i^{\mathrm{th}}$ note. A very simple annotation format was chosen: a list of metrical onsets, measured in quarter notes, with pointers to the corresponding notes in $N$. There are three types of annotations: an onset, a grace note and a rest. Although this study will ignore grace notes and rests, they are included for completeness and potential future use of the corpus.

An annotation $A$, corresponding to note list $N$ has the following format:
\begin{align*}
A &= [a_0, a_1, \cdots, a_N],\\
a_j &= (\mathrm{Position}_j, \mathrm{Pointer}_j, \mathrm{Type}_j),
\end{align*}
where $\mathrm{Position}_j$ is a metrical position, measured in quarter notes, $\mathrm{Pointer}_j$ is a pointer that points to the index of the corresponding note in the note list and $\mathrm{Type}_j$ indicates whether this annotation is an onset, a grace note or a rest. Since rests are not included in the note list, their pointer is irrelevant and points to zero. 

Some extra information is added to the annotation when it is stored. The annotation $A$ is stored in a 3-tuple $(T, t, A)$, where $t$ is the tempo in beats per minute and $T$ is the time signature. The time signature contains the number of beats per measure, or measure division $d$ and the number by which we have to divide 1 to get the units of those beats $u$, where $u$ can be any positive real number (although $u$ is almost always a power of 2). 
\begin{equation*}
T = (d, u)
\end{equation*}
This representation is identical to the musical representation of time signature. The information in the time signature combined with onsets measured in quarter notes can be used to derive the measure number of every onset in the annotation. To do so, quarter notes ($q$) are first converted to beats ($b$) as follows: $b = qu/4$. A position measured in beats can then be converted to a position measured in bars ($m$) like so: $m = b/d$

Many performances in the corpus where played in `swing'. Although swing may refer to many intentional deviations from metronomic timing, a very common one is to play notes that are notated as eighth notes as eighth note triplets, the so-called `shuffle'. This is illustrated in figure \ref{fig:swing}. The score usually notates swung notes as in figure \ref{fig:swing:a}, whereas the notes are played as in figure \ref{fig:swing:b} and \ref{fig:swing:c}. Writing swung notes as in \ref{fig:swing:a} is just a notational convention, and often the scores contain instructions to play eighth notes as in figure \ref{fig:swing:b}.

The manual annotation process resulted in lists of metrical onset times. Combined with the time signature and tempo information, the metrical onset times can be converted to any metrical unit and to metronomic onset times. Deriving te subdivision trees from this information was done semi-automatically. A simple parser with a flat prior and a likelihood function that only allowed metronomic timing was used to generate parses for every item in the corpus. From these results correct parses where selected by hand. A subdivision tree for the swing example is shown in figure \ref{fig:swing:c}. 


\begin{figure}
\centering
\subfloat[Rhythm notation]{
\label{fig:swing:a}
\centering
\begin{tabular}{|c|}
\hline
\includegraphics[scale=0.3]{img/aint_misbehavin}\\
\hline
\end{tabular}
}
\\
\subfloat[Annotation]{
\label{fig:swing:b}
\centering
\begin{tabular}{|c|}
\hline
\includegraphics[scale=0.3]{img/aint_misbehavin_swung}\\
\hline
\end{tabular}
}
\\
\subfloat[Parsetree]{
\label{fig:swing:c}
\centering
\Tree 
[ .{$\frac{1}{1}$} [ .{$\frac{1}{2}$} [ .{$\frac{1}{4}$} [ .$*$ ] [ .$*$ ] [ .$\bullet$ ] ] [ .{$\frac{1}{4}$} [ .$\bullet$ ] [ .$*$ ] [ .$\bullet$ ] ] ] [ .{$\frac{1}{2}$} [ .{$\frac{1}{4}$} [ .$\bullet$ ] [ .$*$ ] [ .$\bullet$ ] ] [ .$*$ ] ] ]
}
\caption{Swung notes.}
\label{fig:swing}
\end{figure}

The result of this process for Thelonious Monk's standard Blue Monk is shown in figure \ref{tab:annotation}. Combining pitch information with our subdivision trees allows us to generate scores. Note that our subdivision trees do not represent harmonic information, so even though this transcription should be in the key B-flat, the score does not contain a key signature. A full list of jazz standards in the corpus is given in appendix \ref{app:corpus}.

\begin{figure}[h!]
\begin{tabular}{|l|}
\hline

\parbox{\linewidth}{
The performance onsets in milliseconds:
}\\

$P = [32, 348, 504, 836, 1940, 2240, 2420, 2728]$\\


\parbox{\linewidth}{
Metrical onsets in quarter notes. Triple divisions are rounded to two digits:
}\\

$ A = [0.0, 0.66, 1.0, 1.66, 4.0, 4.66, 5.0, 5.66]$\\


\parbox{\linewidth}{
Rhythmic analysis generated by a simple parser and selected by hand:
}\\

\Tree
[ .{$\frac{1}{1}$} [ .{$\frac{1}{2}$} [ .{$\frac{1}{4}$} [ .{$\frac{1}{8}$} [ .$\bullet$ ] [ .$*$ ] [ .$\bullet$ ] ] [ .{$\frac{1}{8}$} [ .$\bullet$ ] [ .$*$ ] [ .$\bullet$ ] ] ] [ .$*$ ] ] [ .{$\frac{1}{2}$} [ .{$\frac{1}{4}$} [ .{$\frac{1}{8}$} [ .$\bullet$ ] [ .$*$ ] [ .$\bullet$ ] ] [ .{$\frac{1}{8}$} [ .$\bullet$ ] [ .$*$ ] [ .$\bullet$ ] ] ] [ .$*$ ] ] ]
\\


\parbox{\linewidth}{
Score generated from the subdivision tree combined with pitch information. The bar duration was set to level 1/2 of the subdivision tree.}\\
\includegraphics[scale=0.3]{img/blue_monk}\\
\hline
\end{tabular}
\caption{A performance of the first two bars of Thelonious Monk's Blue Monk, the corresponding metrical onsets, rhythmic analysis and score generated from the analysis.}
\label{tab:annotation}
\end{figure}


\section{Training}
\label{sec:training}

The expression-aware model is trained using maximum likelihood estimation. First, for all items in the train set, the observed parameters are and their corresponding feature vectors, the resulting feature vector/parameter pairs $(\phi, p)$ are stored in a set S. Second, we train two parameter vectors, $\mu$ and $\sigma$ to contain the expected expression ratio $r$ and standard deviation of $r$ for every feature vector $\phi$ like so:
\begin{align}
\label{eq:training}
\mu_\phi &= \frac{1}{|S|} \sum_{(p | (p, \phi) \in S))} p, \\ 
\sigma_\phi &= \frac{1}{|S|} \sum_{(p | (p, \phi) \in S)} (\mu - p)^2. \notag
\end{align}
Since we use a very simple feature vector, these parameters do not need to be smoothed.

Given the parameter vectors $\mu$ and $\sigma$, estimated by \ref{eq:training}, the likelihood of a set of observations $S$ containing feature/expression ratio pairs $(\varphi, r)$, observed in some hypothesis $h$ is given by:
\begin{equation}
\label{eq:h_likelihood}
\mathcal{L}(S|\mu, \sigma) \propto \prod_{(\varphi, r) \in S} \exp\left(-\frac{(\mu_\varphi - r)^2}{2\sigma_\varphi^2}\right).
\end{equation}

As mentioned in section \ref{sec:rejection}, a hypothesis is rejected if the per-item likelihood is lower than a certain threshold, the beam parameter. The per-item likelihood is defined as:

\begin{equation}
\label{eq:per_obs_likelihood}
\mathcal{L}(S| \mu, \sigma) \mbox{ per item } \propto \exp\left(-\frac{1}{|S|}\sum_{(\varphi, r) \in S} \frac{(\mu_\varphi - r)^2}{2\sigma_\varphi^2}\right).
\end{equation}

The beam parameter is set by taking the minimum value of the per-item likelihood of each subdivision tree in the train set. 

Our alternative expression model treats expressive deviation as noise. Deviations from idealised onsets cause expression ratios to be non-zero. The alternative expression model assumes that expression ratios are normally distributed with a mean of zero. 

\section{Implementation}
\label{sec:implementation}

To make the parser computationally tractable, a few optimisations were needed. It was already mentioned that the parser uses a beam parameter to reject unlikely hypotheses. In addition to that a few extra measures had to be taken.

First of all, the \textsc{observations} function returns no parameters for hypotheses containing a single onset (complex onsets). Their probability is always one and theoretically ties could be added endlessly. To restrict this the parser maintains a small list of allowed single note analyses. This list is compiled by taking all single-note analyses found in the labelled corpus.

Second of all, the corpus contains only 4/4 time signatures, therefore triple divisions are allowed only at note level. All allowed triple divisions are shown in figure \ref{fig:triples}. 

\begin{figure}
$\bullet$
\Tree
[ .{$\frac{1}{1}$} [ .$*$ ] [ .$\bullet$ ] ] 
\Tree
[ .{$\frac{1}{1}$} [ .$*$ ] [ .$*$ ] [ .$\bullet$ ] ] 
\Tree
[ .{$\frac{1}{1}$} [ .$*$ ] [ .{$\frac{1}{2}$} [ .$*$ ] [ .$\bullet$ ] ] ] 
\Tree
[ .{$\frac{1}{1}$} [ .$*$ ] [ .{$\frac{1}{2}$} [ .{$\frac{1}{4}$} [ .$*$ ] [ .$\bullet$ ] ] [ .$*$ ] ] ] 
\caption{Some of the allowed single-note analyses.}
\label{fig:singlenotes}
\end{figure}

\begin{figure}
\Tree
[ .{$\frac{1}{1}$} [ .$*$ ] [ .$*$ ] [ .$\bullet$ ] ] 
\Tree
[ .{$\frac{1}{1}$} [ .$\bullet$ ] [ .$*$ ] [ .$\bullet$ ] ]
\Tree 
[ .{$\frac{1}{1}$} [ .$\bullet$ ] [ .$\bullet$ ] [ .$\bullet$ ] ]
\caption{Allowed triple divisions.}
\label{fig:triples}
\end{figure}
% Beam
% Triple divisions
% Best-n
% Max depth
%
% Chart parsing, beam, prior, likelihood, 

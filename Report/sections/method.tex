\section{Method}
\label{sec:method}

\subsection{Data and annotation}

Jazz and Latin standards. Scraped from the internet. Performances of varying quality. Exclusively monophonic melody tracks. Melody tracks were performed on midi instruments. They were assigned to be played with different instruments including piano, saxophone and trumpet. All the melodies are played assumed to be played by humans with the help of a metronome. The constraints to determine human performances where that the performance should have have a minimum variation in time differences between note onsets and a minimum variation in velocities with which the notes were played.

Every note in the performance is annotated with its position, measured in quarter notes. Special elements are inserted for rests and grace notes. Grace notes are annotated assigned the same position as the notes they belongs to. Annotation was done using scores from various real books. Expression and deviation from these scores was made explicit in the annotations as much as possible; if three quarter notes are played as a triplet, they are annotated as a triplet. When the metrical positions of notes in the performance are unclear the annotations follow the score. Rests are inserted as notated in the score. The end of the annotations is marked by an end marker

The metrical length of a note can be derived by subtracting its metrical position from the metrical position of the next element in the annotations. The onset time of a rest is can be derived by multiplying the local 

\[A = \{a_0, a_1, \cdots, a_N\}\]
\[N = \{n_0, n_1, \cdots, n_N\}\]
\[a_i = (\mathrm{Beat}_i, \mathrm{Pointer}_i, \mathrm{Type}_i)\]
\[n_i = (\mathrm{On}_i, \mathrm{Off}_i, \mathrm{Pitch}_i, \mathrm{Velocity}_i)\]


Possible models include:
\begin{itemize}
\item Temperley's unified probabilistic model for polyphonic music analysis
\item Temperley's common-practice rhythm models
\item Potential other models in the literature
\item A CKY-like parsing algorithm 
\item Any other approach that includes performance information
\end{itemize}

Corpora:
\begin{itemize}
\item Essen folk (Used in common-practice rhythm)
\item Kostka-Payne (Used in UPMPMA
\item Jazz corpus (my own)
\end{itemize}

\subsection{Definitions}

We consider a rhythm to be a list of note onsets. We ignore note offsets, even though in many cases, note offsets are rhythmically timed as well, note onsets seem to be the most important property of rhythms and also the property that rhythms played by any instruments have in common (it is hard for example to define the note offsets of a drum line). When note onsets form a rhythm, we can assume that there is a set of units, or metrical durations that form are the atomic units that the onset times are build off. These metrical durations are mostly constrained to be duple or triple divisions of each other. Other divisions are allowed as well but are far less common. When a metrical unit is divided into two or three units, the leftmost unit is, by convention, called the downbeat. The other unit(s) are called upbeat(s).

The task of finding rhythmic structure can now be defined as finding the appropriate atomic units and finding the location of the downbeats. When humans perform a rhythm, some information about this underlying structure is encoded in the performance of the rhythm which may be why humans find it usually easy to hear the downbeat. In many music styles, it is common to emphasise the downbeat. It is hypothesised here that this emphasis takes the form of a slight asymmetry in the duration of the downbeat and the duration of the upbeat.

\subsection{Parser}

A parser is presented here that forms hypotheses about the atomic units that make up a rhythm. The parser is essentially a stochastic CKY parser that combines hypotheses about note onsets. The parser uses a small set of rules and two constraints that restrict it to form only valid rhythmic structures. 

\begin{align*}
\label{eq:rules}
D(\textrm{combine}(h_1, h_2)) &\rightarrow D(h_1) D(h_2)\\
D(\textrm{combine}(h_1, h_2)) &\rightarrow D(h_1) D(h_2) D((h_3)\\
D(\textrm{h}) &\rightarrow \textrm{On}_i\\
D(\textrm{h}) &\rightarrow *
\end{align*}
where $D(\phi)$ is a metrical duration with features $\phi$, $*$ represents a filler duration (this is needed when notes are tied together or when the first note is not the first beat of a measure) [SHOW SOME EXAMPLES HERE], $\textrm{On}_i$ is an onset at index $i$, $h$ is an hypothesis about the rhythmic analysis of the corresponding metrical duration and $\textrm{combine}/2$ and $\textrm{combine}/3$ are functions that take hypotheses and combine them into a new hypothesis.

The two constraints are:
\begin{enumerate}
\item Any set of two or three metrical durations are not allowed to combine if the first one expands directly to an onset and the others do not recursively expand to an onset.
\item Any set of two or three metrical durations is not allowed to combine if none of the recursively contains an onset.
\end{enumerate}

The parser works like any other stochastic CKY parser where symbols are added to the chart only if their probability is higher than a certain threshold.

The performance of the parser is determined in the first place by how a onsets are translated into hypotheses and how hypotheses are combined and in the second place by how the probability of a certain hypothesis is determined. When the probability function would simply assign the probability 1.0 to each hypothesis, the parser would generate an infinite number of hypotheses. The following sections will introduce hypothesis generation and combination functions for inputs with metronomic timing.

\subsection{Hypothesis generation and rejection}

\subsection{Metronomic onset hypothesis generation}

When we assume the onsets are metronomic, we can represent hypotheses in a compact and efficient format. Apart from metronomic onsets we need to make a few further assumptions: the parser can only come up with a valid analysis if we assume that (1) the time $t=0$ corresponds to the first beat of the time signature and that (2) there is an end marker which is placed at the $n$-th bar, where $n$ is the first power of two after the index of the last bar of the rhythm. For example, if we have a 3-bar rhythm, the end marker is placed at the first beat of the 4th bar.

Let us define the parser input as rhythm $R$, consisting of a list of onset-times, the last of which is not an onset but an end-marker. 

\begin{equation}
R_{0,n} = [\textrm{On}_0, \textrm{On}_{1}, \cdots, \textrm{On}_n]
\end{equation}

Before we can parse this rhythm we convert the rhythm $R_{0,n}$ to input $I_{0,n-1}$ (excluding the end marker at $R_n$). This conversion converts each onset to 3-tuple:

\begin{equation}
I_i = (R_{i-1}, R_i, R_{i+1}),
\end{equation}
containing the previous onset, the current onset and the next onset. At $I_0$, the previous onset is assumed to be $0$, at $I_{n-1}$ the next onset is $R_n$, which is the end-marker. 

An hypothesis is represented as a set of features associated with the corresponding duration symbol $D$. Each duration symbol will have at least two: the first onset in the symbol, $I_i$ and its position within the symbol. When a duration symbol contains more than one onset, a third feature is added that contains the length of the symbol that is implied by the onsets in the symbol.

\subsection{Prior}
\subsubsection*{PCFG}
\subsubsection*{Hierarchical position}
\subsection{Expression}




\subsection{Data preparation}


\subsection{Chart Parsing/Statistical Longuet-Higgins}
\label{sec:chart}

A subdivision rhythmic analysis is useful because it allows us to calculate onset likelihoods given that we know where down- and upbeats are located at each level.

Introducing triple divisions introduces ambiguities between triple divisions or more complicated constructions of duple divisions (that are quite unlikely).







\subsection{Expressive onset hypothesis generation}

When onsets are not guaranteed to be metronomic we need a different way of representing hypotheses. It is particularly important where we predict future onsets to be given what we have already seen.

For this we extend the duration symbol with three optional features. Every symbol now has a vector of onsets or ties. The first item of this vector is defined to be the downbeat, the rest are upbeats. The interval of the downbeat and the first upbeat at level $l$ is the downbeat interval at level $l+1$. This notion is the basis of the model. As soon as a vector contains more than one onset, the vector is filled in with expected onsets given the two onsets.
\subsection*{Probabilistic parsing}

A probabilistic model of rhythm can be formulated in a Bayesian way. 

\begin{equation}
\label{eq:model}
P(A|N) \propto P(N|A)P(A),
\end{equation}

where $A$ is a rhythmic analysis and $N$ is a list of onset times. The goal of the analysis is to find the analysis that maximises the formula above.

Equation \ref{eq:model} contains two factors: the likelihood of a note pattern given an analysis $P(N|A)$ and the prior probability of the note pattern $P(A)$. 

We would like to use our corpus to estimate both probabilities. The prior can be established using a simple PCFG. [Explain somewhere why a PCFG captures Temperley's properties of common practice rhythm.] The likelihood of an analysis is more complicated.

Most models use a tempo curve and a normal distribution that penalises deviation from metronomic tempo. Since we have structural information about the rhythm, like where downbeats are located and since this structural information likely correlates with certain expressive properties of rhythm. We may be able to use this to our advantage. 

Many models use tempo curves to determine

Other authors have suggested balancing simplicity and accuracy. We can now elaborate simplicity as the degree to which a rhythmic structure is similar to what we have seen in a corpus and accuracy as the degree to which the onsets generated by the structure are similar to what we would expect.

Many models consider the expressive timing to be Gaussian noise relative to the tempo curve. Since we have structural information we can see if there is a correlation between structure and expressive timing. Often, in such models, the prior is measured as the complexity of the score. Our analysis allows us to determine the likelihood of the rhythmic structure itself. 



\subsubsection{Algorithm}


An hypothesis is represented as a tree node with a list of beats. The list of beats contains onsets or expected onsets of the downbeat of each of its children. When a duration node is a terminal node the list of beats simply contains the onsets that the node contains.

As soon as a node governs two or more onsets, empty slots are filled in. An hypothesis containing two or more nodes always implies a downbeat. When these hypotheses are combined with other nodes, onsets or ties
The hypothesis combination process works by filling in as follows. An hypothesis can be an onset, a tie or a combination of hypotheses. When an hypothesis governs only one onset and one or more tie, the hypothesis is treated as a \textit{complex onset}. The hypothesis $(*$ $\bullet)$ for example is treated as an onset at relative position $0.5$, the hypothesis $(*$ $(*$ $\bullet))$ is treated as an onset with relative position 0.75.

The goal of the combination process is to get the best possible estimates of where the up and downbeats of an hypothesis are. 
\begin{algorithm}
\caption{Combine hypotheses}
\label{alg:hypotheses}
\begin{algorithmic}
\Function{combine}{$H$}
	\State $d \leftarrow$ length($H$)
	\State onsets = []
	\State complexonsets = []
	\For{$i \leftarrow 0,d$}
		\If{isOnset($H[i]$)}
			\State \textbf{append} ($i, H[i]$) \textbf{to} onsets
		\EndIf
		\If{hasChildren(H[i])}
			\State $d \leftarrow$ length($H[i]$.children)
			\State $p$, complexOnset $\leftarrow H[i]$
			\If{H[$i$].beats[0] $\neq$ \textit{Null}}
			\Comment{Check if the hypothesis suggests a downbeat}
				\State \textbf{append} ($i, H[i$].beats[0]) \textbf{to} onsets
			\Else
				\State \textbf{append} ($i + p$, complexOnset) \textbf{to} complexonsets
			\EndIf
		\EndIf
	\EndFor
	\If{length(onsets) <= 1}
		\State \textbf{append} complexOnsets \textbf{to} onsets
	\EndIf
	\State beats $\leftarrow$ \Call{fill}{onsets, $d$}
	\State children = $H[:]$
	\State $h \leftarrow$ children, (onsets$[0][0]/d$, onsets$[0][1]$), beats
	\State \textbf{return} $h$
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Fill beat matrix}
\label{alg:hypotheses}
\begin{algorithmic}
\Function{fill}{beats}
\EndFunction
\end{algorithmic}
\end{algorithm}

So a hypothesis is a tuple containing its children, a complex onset and its estimated beats.
Every hypothesis also keeps track of the first onset before and after the hypothesis. This is used later to restrict adding ties to the hypothesis.

Getting observations


A rhythmic analysis containing two or more notes makes some sort of prediction about where other onsets are likely to appear. 

The observations function is defined recursively. The basic intuition is that for every duration symbol, the likelihood of the upbeats is calculated given the downbeats of the level above. 

\begin{algorithm}
\caption{Generate observations}
\label{alg:observations}
\begin{algorithmic}
\Function{observations}{$h$, downbeat, nextDownbeat}
	\State childNodes, beats $\leftarrow h$.children
	\If {beats[0] $\neq$ \textit{Null}}
		\State downbeat $\leftarrow$ beats[0]
	\EndIf
	\State $d \leftarrow$ length(childNodes)
	\State $l \leftarrow$ nextDownbeat - downbeat
	\State obs $\leftarrow$ []
	\If {hasChildren(childNodes[0])}
		\State upbeat $\leftarrow l/d$
		\If{beats[1] $\neq$ \textit{Null}}
			\State upbeat $\leftarrow$ beats[1]
		\EndIf
		\State obs $\leftarrow$ \Call{observations}{childNodes[0], downbeat, upbeat}
	\EndIf
	
	\For {$i \leftarrow 1,d$}
		\If {beats[$i$] $\neq$ \textit{Null}}
			\State \textbf{append} \Call{features}{downbeat, nextDownbeat, beat, $i, d$} \textbf{to} obs
		\EndIf	
		\If {childNodes[$i$] $\neq$ \textit{Null}}
			\State \textbf{append} \Call{observations}{childNode[$i$], $i/d * l$, nextDownbeat} \textbf{to} obs
		\EndIf
	\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Calculate the ratio of downbeat length and upbeat length. Other features could be calculated here.}
\label{alg:features}
\begin{algorithmic}
\Function{features}{downbeat, nextDownbeat, beat}
	\State \textbf{return} $\frac{((\mathrm{beat} - \mathrm{downbeat}) / i)}{((\mathrm{nextDownbeat} - \mathrm{beat}) / (d - i))}$
\EndFunction
\end{algorithmic}
\end{algorithm}

Calculating likelihood

We can now calculate the likelihood of any hypothesis by taking the product of the observations that it implies. Optionally we can add an expression parameter that suggests the ratios to be slightly larger than one.

\begin{equation}
\label{eq:h_likelihood}
\mathcal{L}(O|h, \mu, \sigma) \propto \prod_{i=0}^N \exp\left(-\frac{(\mu - \log(O_i))^2}{2\sigma^2}\right)
\end{equation}

Where $O$ is a set of down-/upbeat ratios. We expect the down-/upbeat ratios to be close to one and their logarithm close to zero. Therefore it makes sense to set $\mu$ to zero. However we may expect downbeats to be slightly stretched and we can set $\mu$ to reflect this.

Rejecting hypotheses

An hypothesis is rejected if the per item likelihood is lower than a certain threshold. The per-observation likelihood is defined as

\begin{equation}
\label{eq:per_obs_likelihood}
\mathcal{L}(O|h, \mu, \sigma) \mathrm{ per observation } \propto \exp\left(-\frac{1}{N}\sum_{i=0}^N \frac{(\mu - \log(O_i))^2}{2\sigma^2}\right)
\end{equation}


%Longuet-Higgins used a constant tolerance parameter to determine whether a beat should be subdivided or a rest should be generated. Increased computational power and availability of labelled data now allows us to makes these decisions probabilistic.

%\begin{align*}
%&P(A \rightarrow B, C|O) = P(N|A \rightarrow B, C)P(A \rightarrow B, C)\\
%&P(A \rightarrow b|O) = P(N_i|A \rightarrow b)P(A \rightarrow b)
%\end{align*}

%where A, B, C are arbitrary non-terminal symbols, b is a terminal symbol and O is a set of observations. 

%The priors are given by for example a simple PCFG-like model, below we will only discuss likelihoods. Consider a bottom up chart-parsing algorithm. Such an algorithm would for example consider the (possibility that the first note in the following pattern was generated by the rule $B/4 \rightarrow n$, which is, in this case, correct. 
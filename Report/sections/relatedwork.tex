\chapter{Related Work}
\label{sec:relatedwork}

\begin{itemize}
\item Rhythm quantisation, rhythm models
\end{itemize}


Several authors have suggested that some rhythmic structures are more likely than others. \cite{cemgil2000rhythm} incorporated a notion of rhythmic complexity into their system for music transcription, where the likelihood of a rhythm is inversely proportional to its complexity. Temperley suggests a more thorough description of rhythm likelihood \citep{temperley2010modeling}. In \cite{temperley2009unified} he uses his hierarchical position model in a probabilistic music analysis system. [Explain somewhere why a PCFG captures Temperley's properties of common practice rhythm. If it even does...]. [Honing suggests a few priors somewhere else as well.]

Our hypotheses are represented as subdivision trees. This analysis suggests yet another way of characterising rhythm likelihood, commonly known as a probabilistic context-free grammar(PCFG). 

We will use several priors to evaluate our model. [these priors should be described in detail either here or in the evaluation section]

The likelihood should be a function that measures how well a hypothesis `fits' the observations. The simplest likelihood function is one that assigns zero probability to all hypotheses that do not exactly fit the observations. Such a likelihood would only allow metronomic performances. 

If we want to handle human performances we need to have tolerance for deviations from metronomic timing. In many models [citations], any deviation from metronomic timing is treated as additive noise. Deviations from metronomic timing are penalised using a normal distribution centred around the metronomic onset.

Finding the meter of a piece of music is slightly more complicated than just tracking beats. If we want to know the meter we have to know how many beats are in a bar and which beat is the first beat of a bar. Even if we know all these variables, there can still be an ambiguity between a 6/8 and a 3/4 meter. Furthermore, if we assume that rhythms are performed in perfect time, finding the meter is still nontrivial \cite{temperley2010modeling}. In performances however, rhythms are always performed with some level of expressive timing, that is the length and onset of notes is slightly altered with respect to its notation.

A number of models related to the problem of meter finding have been proposed. Early rule based approaches, like \citet{longuet1976perception}, required manual setting of parameters and were not able to benefit corpora of music. As MIDI allowed music to be represented symbolically and data became more available, models have been presented that can learn from data \citep{cemgil2000rhythm, raphael2002hybrid}. In these models, expression is modelled by allowing the tempo to change and allowing note onsets to be slightly before or metric beats. Although the parameters of these models are learned from data, they do not really contain any model of expression. This may be explained by the fact that they are aimed at classical music, where expression can be highly complex and unpredictable.

Using Bayesian modeling has recently become popular. Temperley proposes a number of models aimed to capture properties of common practice rhythm \citep{temperley2010modeling}. In \citet{temperley2010} these models are objectively compared. Two models, the hierarchical model and the first-order metrical duration model seemed to most effictively capture common-practice rhythm. Below, the hierarchical model will be introduced.

The hierarchical model is a generative model of rhythm production. Temperley's model adopts of the widely used notion of metrical grid \citep{lerdahl1983generative}, where beats are considered to be on different metrical levels. In a four layer metrical grid for example, the length of a bar would be level 3 (whole notes in a 4/4 meter), the first subdivision would be level 2 (half notes in a 4/4 meter), the second level 1 and the third level 0. Temperley's hierarchical model first generates beats on level 3, according to statistics obtained from a corpus, then level two is generated in the same way, level 1 and 0 are generated conditional on their context, that is whether they are preceded or followed by a note on a strong beat (level 3 or 4). Temperley distinguishes four different contexts. Non-anchored notes are not preceded or followed by a note on a strong beat, pre-anchored notes are preceded by a note on a strong beat, post anchored notes are followed by a note on a strong beat and both-anchored notes are preceded and followed by a note on a strong beat. The likelihood of generating level 2 or level 1 notes is now given by the conditional probability of a note on that level given its context.

The hierarchical model is based on intuitions about common-practice rhythm \citep{temperley2010modeling}. Temperley observes for example that notes often fall on strong beats of the metrical grid (the lower levels), notes that fall on strong beats tend to be long and notes on weak beats are often preceded by notes on strong beats. In \citet{temperley2009unified}, a meter-finding application of the hierarchical model is presented. However, this model still does not assume a lot of knowledge about expression and simply penalises notes proportionally to how far their offset is from the metric beat. Again, such a simplification may be justified in the light of highly complex expressive timing in classical music.

The story for Jazz music is quite different. Tempo for example, tends to be quite stable in Jazz performances as it is often played in ensemble and without a conductor. It also seems like it may be possible to make generalisations about the expressive timing in Jazz. Jazz is often played in what is known as \textit{swing} timing. That is, the length of 1/8 notes played unequally and typically described in \textit{swing ratio}. For example, a swing ratio of 2/1 means the 1/8 note on a strong beat (quarter beat) is played with 2/3 of the quarter beat length and the next 1/8 note on a weak beat is played with 1/3 of the quarter beat length.

Exactly how to model a swing ratio is not clear. Empirical studies on professional Jazz percussionists have shown that the swing ratio varies with tempo. \citet{friberg2002swing} suggested that swing ratio scales lenearly with tempo. However, this claim was criticised by \citet{honing2008swing}, who provided evidence that swing ratio does scale with tempo but not linearly. This may be related to claim that expressive timing does not scale linearly with tempo \citep{desain1993tempo, desain1994does}.

Finally, it may be suggested that a simultaneous harmonic analysis may improve the rhythmic analysis. \citet{temperley2009unified} describes how such a combined analysis might be implemented. This may be a fruitful approach but as can be seen in Temperley's work, an independent rhythmic analysis can be easily integrated in such a system. In addition to that, humans typically are able to determine rhythmic structure without hearing the harmony, whereas determining the harmonic structure without the rhythm may be harder. For these reasons, the approach proposed here will focus only on rhythm.